\chapter{评估}
在本节中，我们进行全面评估以回答以下问题：
• RQ1：LLMalMorph 生成的恶意软件变体对广泛使用的防病毒引擎和机器学习分类器的检测具有多强的抗性？其规避能力与最近的对抗性恶意软件生成框架生成的变体相比如何？
• RQ2：不同转换策略间的代码编辑工作量有何差异？这对揭示 LLM 所犯错误类型有何启示？
• RQ3：生成的恶意软件变体是否保留了原始样本的语义和功能？

\section{评估设置}

\subsection{选择样本}
由于最新的恶意软件源代码稀缺，大多数恶意软件研究都集中在可执行文件上。我们检查了公共数据库 [38], [39]，发现大多数可用的 Windows 恶意软件源代码是 32 位的，因此我们将研究重点放在 32 位变体上。我们选择了能够编译成可运行可执行文件、并表现出可通过 VirusTotal 或 Hybrid Analysis 检测到的恶意行为（其 AV 检测率≥60\%）的样本。这产生了十个具有不同复杂性和类型的恶意软件候选样本。RansomWar 样本使用 GCC 编译，其余样本使用 Microsoft Visual Studio 2022 编译（因为提供了 .sln 文件）。表 II 总结了选定样本的关键细节。对于 Conti 和 Babuk 勒索软件，我们的分析集中在负责加密的加密器组件上。有关样本的详细信息请参见附录 B。

\subsection{评估指标}
我们使用给定的评估指标：防病毒（AV）检测率（$R^{\hat{Ms}}$）。我们使用 VirusTotal 评估 AV 检测率，该平台使用来自不同供应商的多个 AV 引擎扫描样本。每个样本可用的检测器数量因可用性而异。令 $D$ 为可用检测器的集合，$\hat{D} \subseteq D$ 为将恶意软件变体标记为恶意的检测器集合。在第 k 次运行时，变体 $\hat{M_{s}}$ 的 AV 检测率 $R^{\hat{M_{s}}}_{k}$ 定义为 $R^{\hat{M_{s}}}_{k} = \frac{|\hat{D}|}{|D|} \times 100$，其中 $|.|$ 表示两个集合的大小。

每个样本将进行$k$次运行，并计算平均检测率$R^{\hat{M_{s}}}=\frac{1}{k} \sum_{i=1}^{k} R_{k}^{\hat{M_{s}}}$。我们在实验中设定 k=3 以解释不同运行间的变异性。我们还使用 Hybrid Analysis，该工具包含静态分析、基于机器学习的分析以及使用不同引擎的多重扫描分析。检测性能以每个恶意软件样本及其变体在 k 次运行上的平均百分比表示。我们使用这两个工具的免费版本，并通过其各自的 API 自动化样本上传、结果检索和后处理。